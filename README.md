## Visual Question and Answering

This work is conducted by members of group 15 for the CSI 5386: Natural Language Processing

Johan Fernandes, Nima Taherifard

For this project we tried to understand the inner workings of the state-of-the-art VQA models.
In our research we studied the components of 4 models. Due to lack of a GPU we could not implement
these models on the complete dataset. 
Thus we did our best to explain the important components of these models and what makes them
better than the rest. Thus our github repo is simply a collection of steps on how to use the models
whose code base was available.

The attached report will give you a brief understanding of each of these models. The top 3 models that
are described in the document have actually secured the top 3 spot in the VQA 2019 challenge.
The code base for the three models that we have stored here are : MCAN, LXMERT and DFAF. 
Here are the official links or github pages to the model code:
* MCAN: https://github.com/MILVLG/openvqa
* LXMERT: https://github.com/airsplay/lxmert
* DFAF: https://github.com/bupt-cist/DFAF-for-VQA.pytorch

 
We have explained these concepts and also provided more information about attention networks which we hope will be beneficial for
anyone who is interested in Natural Language Processing and Computer Vision

The VQA 2020 challenge is about to end and we hope to learn and add more to this repo and our report
when the results are announced. We hope to see more models using attention networks and to utilize the 
capability of BERT for word embeddings.